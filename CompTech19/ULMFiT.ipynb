{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import io\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         label  \\\n",
      "0  Как заблокировать сим-карту   \n",
      "1  Как заблокировать сим-карту   \n",
      "2  Как заблокировать сим-карту   \n",
      "3  Как заблокировать сим-карту   \n",
      "4  Как заблокировать сим-карту   \n",
      "\n",
      "                                                text  \n",
      "0                                Block this sim card  \n",
      "1  Hello, I want to block this sim card, how to d...  \n",
      "2                            Can I block a sim card?  \n",
      "3                Hello. I want to block the sim card  \n",
      "4                            how to block a sim card  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('intents_en.csv')\n",
    "data['text'] = data['text_en']\n",
    "data.drop(['text_en', 'marker'],axis=1, inplace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kolsha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         label  \\\n",
      "0  Как заблокировать сим-карту   \n",
      "1  Как заблокировать сим-карту   \n",
      "2  Как заблокировать сим-карту   \n",
      "3  Как заблокировать сим-карту   \n",
      "4  Как заблокировать сим-карту   \n",
      "\n",
      "                                                text  \n",
      "0                                Block this sim card  \n",
      "1  Hello, I want to block this sim card, how to d...  \n",
      "2                            Can I block a sim card?  \n",
      "3                Hello. I want to block the sim card  \n",
      "4                            how to block a sim card  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df['text'] = df['text'].str.replace(\"[^a-zA-Zа-яА-Я]\", \" \")\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['label'].values)\n",
    "df['label'] = le.transform(df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1                                Block this sim card\n",
      "1      1  Hello  I want to block this sim card  how to d...\n",
      "2      1                            Can I block a sim card \n",
      "3      1                Hello  I want to block the sim card\n",
      "4      1                            how to block a sim card\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "# tokenization \n",
    "tokenized_doc = df['text'].apply(lambda x: x.split())\n",
    "\n",
    "# remove stop-words \n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization \n",
    "detokenized_doc = [] \n",
    "for i in range(len(df)): \n",
    "    t = ' '.join(tokenized_doc[i]) \n",
    "    detokenized_doc.append(t) \n",
    "\n",
    "df['text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                         text\n",
      "0      1               Block sim card\n",
      "1      1  Hello I want block sim card\n",
      "2      1         Can I block sim card\n",
      "3      1  Hello I want block sim card\n",
      "4      1               block sim card\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation set\n",
    "df_trn, df_val = train_test_split(df, stratify = df['label'], test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((789, 2), (88, 2))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = \"\")\n",
    "\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_df(path = \"\", train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 01:57 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>8.525010</th>\n",
       "    <th>7.734663</th>\n",
       "    <th>0.076116</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>8.511082</th>\n",
       "    <th>7.684545</th>\n",
       "    <th>0.082366</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>8.529038</th>\n",
       "    <th>7.641004</th>\n",
       "    <th>0.077232</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>8.496454</th>\n",
       "    <th>7.602447</th>\n",
       "    <th>0.079911</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>8.473711</th>\n",
       "    <th>7.516505</th>\n",
       "    <th>0.076563</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>8.445696</th>\n",
       "    <th>7.511761</th>\n",
       "    <th>0.079911</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>8.412202</th>\n",
       "    <th>7.422843</th>\n",
       "    <th>0.083036</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>8.402211</th>\n",
       "    <th>7.456094</th>\n",
       "    <th>0.079018</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>8.382835</th>\n",
       "    <th>7.415691</th>\n",
       "    <th>0.082366</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>8.367177</th>\n",
       "    <th>7.378826</th>\n",
       "    <th>0.083259</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the learner object with learning rate = 1e-2\n",
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, drop_mult=0.9)\n",
    "learn.load_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 02:13 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>2.372224</th>\n",
       "    <th>2.277485</th>\n",
       "    <th>0.090909</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>2.274985</th>\n",
       "    <th>1.980194</th>\n",
       "    <th>0.579545</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>2.121575</th>\n",
       "    <th>1.615075</th>\n",
       "    <th>0.659091</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.905067</th>\n",
       "    <th>1.369346</th>\n",
       "    <th>0.693182</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.732610</th>\n",
       "    <th>1.174521</th>\n",
       "    <th>0.704545</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.590267</th>\n",
       "    <th>1.069598</th>\n",
       "    <th>0.761364</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.488598</th>\n",
       "    <th>0.976185</th>\n",
       "    <th>0.738636</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>1.491502</th>\n",
       "    <th>0.954593</th>\n",
       "    <th>0.704545</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>1.412291</th>\n",
       "    <th>0.942735</th>\n",
       "    <th>0.750000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>1.387349</th>\n",
       "    <th>0.950298</th>\n",
       "    <th>0.738636</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds, targets = learn.get_preds()\n",
    "\n",
    "# predictions = np.argmax(preds, axis = 1)\n",
    "# pd.crosstab(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 7 0 0 ... 1 0 7 2]\n",
      "Hello I want block sim card\n"
     ]
    }
   ],
   "source": [
    "print(np.array(targets))\n",
    "print(df.loc[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Category 1, tensor(1), tensor([0.1164, 0.1751, 0.0683, 0.0476, 0.0448, 0.1212, 0.1367, 0.1213, 0.1220,\n",
      "        0.0466]))\n",
      "['Как заблокировать сим-карту']\n"
     ]
    }
   ],
   "source": [
    "text = 'I do not want to talk to the bot, give a living person'\n",
    "pred= learn.predict(text)\n",
    "print(pred)\n",
    "print(le.inverse_transform([int(pred[0])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, random_state=42) #, shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro: 0.1194732411837675\n",
      "F1 Micro: 0.1590909090909091\n",
      "BEST F1 Macro: 0.18349219007113743\n",
      "BEST F1 Micro: 0.2159090909090909\n",
      "AVG  F1 Macro: 0.11472399432925748\n",
      "0.03156019808651784\n"
     ]
    }
   ],
   "source": [
    "f1_mac_best = 0\n",
    "f1_mic_best = 0\n",
    "best_ulm = None\n",
    "f1_mac_all = []\n",
    "for (train, test) in cv.split(df['text'], df['label']):\n",
    "    print(f1_mac_all)\n",
    "    # Language model data\n",
    "    data_lm = TextLMDataBunch.from_df(train_df = df.loc[train], valid_df = df.loc[test], path = \"\")\n",
    "\n",
    "    # Classifier model data\n",
    "    data_clas = TextClasDataBunch.from_df(path = \"\",\n",
    "                                          train_df = df.loc[train],\n",
    "                                          valid_df = df.loc[test],\n",
    "                                          vocab=data_lm.train_ds.vocab, bs=32)\n",
    "    \n",
    "    learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.9)\n",
    "    \n",
    "    \n",
    "    # train the learner object with learning rate = 1e-2\n",
    "    learn.fit_one_cycle(1, 1e-3)\n",
    "    \n",
    "    learn.save_encoder('ft_enc')\n",
    "    \n",
    "    learn = text_classifier_learner(data_clas, drop_mult=0.9)\n",
    "    learn.load_encoder('ft_enc')\n",
    "    \n",
    "    learn.fit_one_cycle(1, 1e-3)\n",
    "\n",
    "    \n",
    "    y_pred = np.array(learn.get_preds()[1])\n",
    "    \n",
    "    f1_mac = f1_score(df.loc[test]['label'], y_pred, average='macro')\n",
    "    f1_mic = f1_score(df.loc[test]['label'], y_pred, average='micro')\n",
    "    \n",
    "    f1_mac_all.append(f1_mac)\n",
    "    print(\"F1 Macro: {}\".format(f1_mac) )\n",
    "    print(\"F1 Micro: {}\".format(f1_mic) )\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    if f1_mac > f1_mac_best:\n",
    "#         best_ulm = copy.copy(learn)\n",
    "        f1_mac_best = f1_mac\n",
    "        f1_mic_best = f1_mic\n",
    "    \n",
    "print(\"BEST F1 Macro: {}\".format(f1_mac_best) )\n",
    "print(\"BEST F1 Micro: {}\".format(f1_mic_best) )\n",
    "\n",
    "f1_mac_all = np.array(f1_mac_all)\n",
    "f1_mac_avg = f1_mac_all.mean()\n",
    "print(\"AVG  F1 Macro: {}\".format(f1_mac_avg) )\n",
    "print(f1_mac_all.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
