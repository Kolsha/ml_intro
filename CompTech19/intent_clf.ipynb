{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is first try intent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import codecs\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import modeling\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tokenization\n",
    "\n",
    "import sys\n",
    "# Импорты из https://github.com/google-research/bert\n",
    "from extract_features import InputExample, InputFeatures, input_fn_builder, model_fn_builder\n",
    "\n",
    "from extract_features import convert_examples_to_features, _truncate_seq_pair, read_examples\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BASE_DIR = '/Users/kolsha/Documents/Projects/Python/BERT/multi_cased_L-12_H-768_A-12'\n",
    "\n",
    "init_checkpoint = BERT_BASE_DIR + '/bert_model.ckpt'\n",
    "\n",
    "layer_indexes = [-1]\n",
    "\n",
    "use_one_hot_embeddings = False\n",
    "\n",
    "max_seq_length = 128\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(BERT_BASE_DIR +'/bert_config.json')\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_BASE_DIR+ '/vocab.txt', do_lower_case=False)\n",
    "\n",
    "for (j, layer_index) in enumerate(layer_indexes):\n",
    "    print(j, layer_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "  master=None,\n",
    "  tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "      per_host_input_for_training=is_per_host)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines_to_examples(lines):\n",
    "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "    examples = []\n",
    "    unique_id = 0\n",
    "    for line in lines:\n",
    "        line = tokenization.convert_to_unicode(line)\n",
    "        if not line:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        text_a = None\n",
    "        text_b = None\n",
    "        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "        if m is None:\n",
    "            text_a = line\n",
    "        else:\n",
    "            text_a = m.group(1)\n",
    "            text_b = m.group(2)\n",
    "        examples.append(\n",
    "          InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "        unique_id += 1\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  init_checkpoint=init_checkpoint,\n",
    "  layer_indexes=layer_indexes,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "  use_tpu=False,\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  predict_batch_size=32)\n",
    "    \n",
    "def get_embeddings(lines):\n",
    "    result = []\n",
    "    \n",
    "    examples = convert_lines_to_examples(lines)\n",
    "    \n",
    "    features = convert_examples_to_features(\n",
    "      examples=examples, seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "    \n",
    "    unique_id_to_feature = {}\n",
    "    for feature in features:\n",
    "        unique_id_to_feature[feature.unique_id] = feature\n",
    "    \n",
    "\n",
    "\n",
    "    input_fn = input_fn_builder(\n",
    "      features=features, seq_length=max_seq_length)\n",
    "    \n",
    "    pred = estimator.predict(input_fn, yield_single_examples=True)\n",
    "    \n",
    "    for p in pred:\n",
    "        #print(p['layer_output_0'].shape)\n",
    "        unique_id = int(p[\"unique_id\"])\n",
    "        feature = unique_id_to_feature[unique_id]\n",
    "        #print(feature.tokens)\n",
    "        layer_output = p[\"layer_output_0\"]\n",
    "        r = np.array([round(float(x), 6) for x in layer_output[0:1].flat])\n",
    "        result.append(r)\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "import random\n",
    "\n",
    "def predict_input(lines, estimator, lb_enc):\n",
    "    hz_answers = [\n",
    "        'Переформулируй пожалуйста',\n",
    "        'Это точно по русски было?',\n",
    "        'Я Вам не смогу помочь',\n",
    "    ]\n",
    "    embeddings = get_embeddings(lines)\n",
    "    pred = estimator.predict_proba(embeddings)\n",
    "    result = []\n",
    "    for p in pred:\n",
    "        a_max = np.argmax(p, axis=None)\n",
    "        if p[a_max] > 0.65:\n",
    "            result.append(lb_enc.inverse_transform([a_max])[0])\n",
    "        else:\n",
    "            result.append(random.choice(hz_answers))\n",
    "            \n",
    "        \n",
    "    return result#lb_enc.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_embeddings(['Тест на ', 'Это был тест на ']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('intents.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'] = data['text'].str.replace(\"[^a-zA-Zа-яА-Я]\", \" \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embeddings = get_embeddings(data['text'].values)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings_old', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('embeddings_old.npy')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['label'].values)\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = le.transform(data['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, random_state=42) #, shuffle=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(\n",
    "    criterion='entropy', n_estimators=140,\n",
    "#     max_features=None,\n",
    "#     max_depth=17,\n",
    "#     min_samples_leaf=2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mac_best = 0\n",
    "f1_mic_best = 0\n",
    "f1_mac_all = []\n",
    "for (train, test) in cv.split(embeddings, Y):\n",
    "    forest.fit(embeddings[train], Y[train])\n",
    "    y_pred = forest.predict(embeddings[test])\n",
    "    \n",
    "    f1_mac = f1_score(Y[test], y_pred, average='macro')\n",
    "    f1_mic = f1_score(Y[test], y_pred, average='micro')\n",
    "    \n",
    "    f1_mac_all.append(f1_mac)\n",
    "    print(\"F1 Macro: {}\".format(f1_mac) )\n",
    "    print(\"F1 Micro: {}\".format(f1_mic) )\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    if f1_mac > f1_mac_best:\n",
    "#         best_logreg = copy.copy(logreg)\n",
    "        f1_mac_best = f1_mac\n",
    "        f1_mic_best = f1_mic\n",
    "    \n",
    "print(\"BEST F1 Macro: {}\".format(f1_mac_best) )\n",
    "print(\"BEST F1 Micro: {}\".format(f1_mic_best) )\n",
    "\n",
    "f1_mac_all = np.array(f1_mac_all)\n",
    "f1_mac_avg = f1_mac_all.mean()\n",
    "print(\"AVG  F1 Macro: {}\".format(f1_mac_avg) )\n",
    "print(f1_mac_all.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST F1 Macro: 0.7565208890443331 <br/>\n",
    "BEST F1 Micro: 0.7840909090909092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mac_best = 0\n",
    "f1_mic_best = 0\n",
    "f1_mac_all = []\n",
    "for (train, test) in cv.split(embeddings, Y):\n",
    "#     forest.fit(embeddings[train], Y[train])\n",
    "#     y_pred = forest.predict(embeddings[test])\n",
    "    \n",
    "    xg_train = xgb.DMatrix(embeddings[train], label=Y[train])\n",
    "    xg_test = xgb.DMatrix(embeddings[test], label=Y[train])\n",
    "    # setup parameters for xgboost\n",
    "    param = {\n",
    "            'n_estimators': 300,\n",
    "            'max_depth': 6,\n",
    "            'objective': 'multi:softmax',\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.9,\n",
    "            'colsample_bytree': 0.9\n",
    "            #'eta': 0.1, \n",
    "            }\n",
    "\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 4\n",
    "    param['num_class'] = len(le.classes_)\n",
    "\n",
    "    watchlist = [(xg_train, 'train')]#, (xg_test, 'test')\n",
    "    num_round = 300\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=30)\n",
    "    # get prediction\n",
    "    y_pred = bst.predict(xg_test)\n",
    "    f1_mac = f1_score(Y[test], y_pred, average='macro')\n",
    "    f1_mic = f1_score(Y[test], y_pred, average='micro')\n",
    "    \n",
    "    f1_mac_all.append(f1_mac)\n",
    "    print(\"F1 Macro: {}\".format(f1_mac) )\n",
    "    print(\"F1 Micro: {}\".format(f1_mic) )\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    if f1_mac > f1_mac_best:\n",
    "#         best_logreg = copy.copy(logreg)\n",
    "        f1_mac_best = f1_mac\n",
    "        f1_mic_best = f1_mic\n",
    "    \n",
    "print(\"BEST F1 Macro: {}\".format(f1_mac_best) )\n",
    "print(\"BEST F1 Micro: {}\".format(f1_mic_best) )\n",
    "\n",
    "f1_mac_all = np.array(f1_mac_all)\n",
    "f1_mac_avg = f1_mac_all.mean()\n",
    "print(\"AVG  F1 Macro: {}\".format(f1_mac_avg) )\n",
    "print(f1_mac_all.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST F1 Macro: 0.819804920434346 <br/>\n",
    "BEST F1 Micro: 0.8522727272727273\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid_xgb = {\n",
    "          'nthread':[4],\n",
    "          'objective': ['multi:softmax'],\n",
    "          'reg_alpha': [0, 0.5],\n",
    "          'reg_lambda': [0, 0.5],\n",
    "          'gamma': [0, 0.5],\n",
    "          'subsample': [0.7, 1],\n",
    "          'colsample_bytree':[0.7, 1],\n",
    "          'max_depth': [1, 9, 20],\n",
    "          'learning_rate': [0.05],\n",
    "          'n_estimators': [100, 500]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, params_grid_xgb, n_jobs=-1, \n",
    "                   cv=cv, \n",
    "#                    scoring='roc_auc',\n",
    "                   verbose=True, refit=True)\n",
    "print(params_grid_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(embeddings, Y)\n",
    "print(\"Fit end\")\n",
    "print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=42, solver='lbfgs',#'lbfgs', 'newton-cg'\n",
    "                            multi_class='multinomial',\n",
    "                            max_iter=3000,\n",
    "#                             C=0.75\n",
    "                           )\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mac_best = 0\n",
    "f1_mic_best = 0\n",
    "best_logreg = None\n",
    "f1_mac_all = []\n",
    "for (train, test) in cv.split(embeddings, Y):\n",
    "    logreg.fit(embeddings[train], Y[train])\n",
    "    y_pred = logreg.predict(embeddings[test])\n",
    "    \n",
    "    f1_mac = f1_score(Y[test], y_pred, average='macro')\n",
    "    f1_mic = f1_score(Y[test], y_pred, average='micro')\n",
    "    \n",
    "    f1_mac_all.append(f1_mac)\n",
    "    print(\"F1 Macro: {}\".format(f1_mac) )\n",
    "    print(\"F1 Micro: {}\".format(f1_mic) )\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    if f1_mac > f1_mac_best:\n",
    "        best_logreg = copy.copy(logreg)\n",
    "        f1_mac_best = f1_mac\n",
    "        f1_mic_best = f1_mic\n",
    "    \n",
    "print(\"BEST F1 Macro: {}\".format(f1_mac_best) )\n",
    "print(\"BEST F1 Micro: {}\".format(f1_mic_best) )\n",
    "\n",
    "f1_mac_all = np.array(f1_mac_all)\n",
    "f1_mac_avg = f1_mac_all.mean()\n",
    "print(\"AVG  F1 Macro: {}\".format(f1_mac_avg) )\n",
    "print(f1_mac_all.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST F1 Macro: 0.9699559699559699\n",
    "BEST F1 Micro: 0.9772727272727273\n",
    "AVG  F1 Macro: 0.9108290028012584\n",
    "0.03344453642061686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    'Здравствуйте, меня зовут Павел, я не понимаю, как можно заблокировать симкарту?',\n",
    "    'СОС, ХЕЛП, нужно срочно блокнуть симку, СРОЧНО',\n",
    "    'Алоха, как восстановить симкарту?', # bad sample\n",
    "    \n",
    "    'Это снова Павел, где мои деньги, негодяи?',\n",
    "    'Ребят, случайно отправил деньги не туда',\n",
    "    \n",
    "    'А ты точно человек, а скажи чтонибудь по человечьи?'\n",
    "]\n",
    "print(predict_input(samples, best_logreg, le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_logreg, \"best_logreg_3_2_19\", compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = get_embeddings(samples)\n",
    "pred = best_logreg.predict_proba(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.set_printoptions(precision=1)\n",
    "# for p in pred:\n",
    "#     a_max = np.argmax(p, axis=None)\n",
    "    \n",
    "# #     ind = np.unravel_index(np.argmax(a, axis=None), a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VK_API_ACCESS_TOKEN = ''  \n",
    "GROUP_ID = 177447412          \n",
    "VK_API_VERSION = '5.74'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vk\n",
    "from requests import *\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "session = vk.Session(access_token = VK_API_ACCESS_TOKEN)\n",
    "api = vk.API(session, v = VK_API_VERSION)\n",
    "\n",
    "\n",
    "longPoll = api.groups.getLongPollServer(group_id = GROUP_ID)\n",
    "\n",
    "server, key, ts = longPoll['server'], longPoll['key'], longPoll['ts']\n",
    "while True:\n",
    "    \n",
    "    longPoll = post('%s'%server, data = {'act': 'a_check',\n",
    "                                         'key': key,\n",
    "                                         'ts': ts,\n",
    "                                         'wait': 1,\n",
    "                                         'version': 2}).json()\n",
    "    ts = longPoll['ts']\n",
    "    if longPoll['updates'] and len(longPoll['updates']) != 0:\n",
    "        for update in longPoll['updates']:\n",
    "            if update['type'] == 'message_new':\n",
    "                print('message_new')\n",
    "                # Помечаем сообщение от этого пользователя как прочитанное\n",
    "                api.messages.setActivity(peer_id = update['object']['user_id'], type='typing', group_id = GROUP_ID)\n",
    "                api.messages.markAsRead(peer_id = update['object']['user_id'])\n",
    "                text = update['object']['body']\n",
    "                text = text.replace(\"[^a-zA-Zа-яА-Я]\", \" \")\n",
    "#                 text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "                name = api.users.get(user_ids = update['object']['user_id'])[0]['first_name']\n",
    "                res  = predict_input([text], best_logreg, le)\n",
    "                api.messages.send(peer_id = update['object']['user_id'],\n",
    "                                  message = '{}'.format(res[0]))\n",
    "                break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = SVC(probability=True,\n",
    "              random_state=42,\n",
    "              gamma='auto',\n",
    "              kernel='linear'\n",
    "              \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mac_best = 0\n",
    "f1_mic_best = 0\n",
    "best_svc_clf = None\n",
    "f1_mac_all = []\n",
    "for (train, test) in cv.split(embeddings, Y):\n",
    "    svc_clf.fit(embeddings[train], Y[train])\n",
    "    y_pred = svc_clf.predict(embeddings[test])\n",
    "    \n",
    "    f1_mac = f1_score(Y[test], y_pred, average='macro')\n",
    "    f1_mic = f1_score(Y[test], y_pred, average='micro')\n",
    "    \n",
    "    f1_mac_all.append(f1_mac)\n",
    "    print(\"F1 Macro: {}\".format(f1_mac) )\n",
    "    print(\"F1 Micro: {}\".format(f1_mic) )\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    if f1_mac > f1_mac_best:\n",
    "        best_svc_clf = copy.copy(svc_clf)\n",
    "        f1_mac_best = f1_mac\n",
    "        f1_mic_best = f1_mic\n",
    "    \n",
    "print(\"BEST F1 Macro: {}\".format(f1_mac_best) )\n",
    "print(\"BEST F1 Micro: {}\".format(f1_mic_best) )\n",
    "\n",
    "f1_mac_all = np.array(f1_mac_all)\n",
    "f1_mac_avg = f1_mac_all.mean()\n",
    "print(\"AVG  F1 Macro: {}\".format(f1_mac_avg) )\n",
    "print(f1_mac_all.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
